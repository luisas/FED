{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed624904",
   "metadata": {},
   "source": [
    "# Enformer human validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafbf0ab",
   "metadata": {},
   "source": [
    "### Load  pre-trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba173e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-28 11:56:45.833117: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-28 11:56:45.833139: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import joblib\n",
    "import gzip\n",
    "import kipoiseq\n",
    "from kipoiseq import Interval\n",
    "import pyfaidx\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import os\n",
    "import enformer \n",
    "from tqdm import tqdm\n",
    "import importlib.util\n",
    "import inspect\n",
    "from typing import Any, Callable, Dict, Optional, Text, Union, Iterable\n",
    "import attention_module\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import h5py\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f7cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils.py as module\n",
    "spec_utils = importlib.util.spec_from_file_location(\"enformer\", os.path.join(os.getcwd() ,\"utils.py\"))\n",
    "utils = importlib.util.module_from_spec(spec_utils)\n",
    "spec_utils.loader.exec_module(utils)\n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6254a",
   "metadata": {},
   "source": [
    "### Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0900f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_path = 'gs://dm-enformer/models/enformer.finetuned.SAD.robustscaler-PCA500-robustscaler.transform.pkl'\n",
    "model_path = 'https://tfhub.dev/deepmind/enformer/1'\n",
    "datadir = \"../../../../data/FED\"\n",
    "outputdir = os.path.join(datadir, \"hd5\")\n",
    "fasta_file = os.path.join(datadir, \"hg38.fa\")\n",
    "human_sequences = os.path.join(datadir, \"data_human_sequences.bed\")\n",
    "pyfaidx.Faidx(fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91386fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Enformer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f9d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import enformer.py as module\n",
    "spec = importlib.util.spec_from_file_location(\"enformer\", os.path.join(os.getcwd() ,\"enformer.py\"))\n",
    "enformer = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(enformer)\n",
    "from enformer import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e380ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_extractor = FastaStringExtractor(fasta_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2f5d7",
   "metadata": {},
   "source": [
    "### Check tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b5360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download targets from Basenji2 dataset \n",
    "# Cite: Kelley et al Cross-species regulatory sequence activity prediction. PLoS Comput. Biol. 16, e1008050 (2020).\n",
    "targets_txt = 'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_human.txt'\n",
    "df_targets = pd.read_csv(targets_txt, sep='\\t')\n",
    "df_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fcffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppl = pd.ExcelFile(os.path.join(datadir, \"enformer_suppl.xlsx\"))\n",
    "print(suppl.sheet_names)\n",
    "suppl_human = suppl.parse(suppl.sheet_names[1])\n",
    "suppl_mouse = suppl.parse(suppl.sheet_names[2])\n",
    "suppl_human[\"organism\"] = \"human\"\n",
    "suppl_mouse[\"organism\"] = \"mouse\"\n",
    "frames = [suppl_human, suppl_mouse]\n",
    "suppl_df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c07dcd",
   "metadata": {},
   "source": [
    "## Example predict one sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc24e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence):\n",
    "    return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n",
    "\n",
    "SEQUENCE_LENGHT = 393_216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e012636",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pad the sequence with Ns (anyways ignored by the model)\n",
    "def pad_one_hot(sequence_one_hot, NEW_SIZE):\n",
    "    ADD_ENDS = int((NEW_SIZE - sequence_one_hot.shape[0])/2)\n",
    "    pad_zero = np.tile(np.array([0., 0., 0., 0.]), (ADD_ENDS, 1))\n",
    "    padded_left = np.append(pad_zero,sequence_one_hot, axis=0)\n",
    "    pad_sequence = np.append(padded_left,pad_zero, axis=0)\n",
    "    return(pad_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f917fd13",
   "metadata": {},
   "source": [
    "### compute score (how well predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21187f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO \n",
    "#1 - retrieve the 197k sequence instead o 131k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a615e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dataset = get_dataset('human', 'valid').batch(1).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_all_sequences(model, dataset, head, max_steps=None):\n",
    "    \n",
    "    metric = MetricDict({'PearsonR': PearsonR(reduce_axis=(0,1))})\n",
    "    print(\"Metric dictionary created\")\n",
    "    \n",
    "    def predict(x):\n",
    "        print(\"Beginning prediction\")\n",
    "        padded_sequence = pad_one_hot(np.squeeze(x.numpy(), axis=0), SEQUENCE_LENGHT)[np.newaxis]\n",
    "        predictions = model.predict_on_batch(padded_sequence)[head]\n",
    "        return tf.convert_to_tensor(predictions, dtype=tf.float32)\n",
    "    print(\"Predict funciton loaded\")\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(dataset)):\n",
    "        if max_steps is not None and i > max_steps:\n",
    "            break\n",
    "        prediction = predict(batch['sequence'])\n",
    "        metric.update_state(batch['target'], prediction)\n",
    "\n",
    "    return metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ddb14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on first ten \n",
    "# Right now it evaluates the whole model and \n",
    "metrics_human = evaluate_model_all_sequences(model,\n",
    "                               dataset=get_dataset('human', 'valid').batch(1).prefetch(2),\n",
    "                               head='human',\n",
    "                               max_steps=2)\n",
    "#print('')dataset_197k\n",
    "#print({k: v.numpy().mean() for k, v in metrics_human.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_all_sequences(model, sequence_dict, head, max_steps=None):\n",
    "    \n",
    "    metric = MetricDict({'PearsonR': PearsonR(reduce_axis=(0,1))})\n",
    "    print(\"Metric dictionary created\")\n",
    "    \n",
    "    def predict(x):\n",
    "        print(\"Beginning prediction\")\n",
    "        padded_sequence = pad_one_hot(np.squeeze(x.numpy(), axis=0), SEQUENCE_LENGHT)[np.newaxis]\n",
    "        predictions = model.predict_on_batch(padded_sequence)[head]\n",
    "        return tf.convert_to_tensor(predictions, dtype=tf.float32)\n",
    "    print(\"Predict funciton loaded\")\n",
    "    \n",
    "    i = 0 \n",
    "    for keys in sequence_dict.keys():\n",
    "        if max_steps is not None and i > max_steps:\n",
    "            break\n",
    "        i = i+1\n",
    "        prediction = predict(sequence_dict['sequence'])\n",
    "        metric.update_state(sequence_dict['target'], prediction)\n",
    "\n",
    "    return metric.result()\n",
    "\n",
    "\n",
    "\n",
    "metrics_human = evaluate_model_all_sequences(model,\n",
    "                               dataset= dataset_197k,\n",
    "                               head='human',\n",
    "                               max_steps=2)\n",
    "#print('')dataset_197k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_197k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9908f61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_human"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd0b39",
   "metadata": {},
   "source": [
    "#### Distributions of pearson correlation coefficients per assay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95bf058a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'suppl_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4048325/1654452723.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# assuming the values are in order of assay (TODO check)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0massay_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuppl_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"assay_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpearson_per_assay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_human\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PearsonR\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massay_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpearson_per_assay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_pearson_assay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_tuples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'assay'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pearson'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'suppl_df' is not defined"
     ]
    }
   ],
   "source": [
    "# assuming the values are in order of assay (TODO check) \n",
    "assay_list = list(suppl_df[\"assay_type\"])\n",
    "pearson_per_assay = list(metrics_human[\"PearsonR\"].numpy())\n",
    "data_tuples = list(zip(assay_list,pearson_per_assay))\n",
    "df_pearson_assay = pd.DataFrame(data_tuples, columns=['assay','pearson'])\n",
    "df_pearson_assay[\"pearson\"]\n",
    "df = df_pearson_assay\n",
    "df = df.astype({\"assay\": str, \"pearson\": float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"assay\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ad2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Initialize \n",
    "g = sns.FacetGrid(df, row=\"assay\", hue=\"assay\", aspect=15, height=1, palette=\"mako\")\n",
    "\n",
    "# Draw the densities in a few steps\n",
    "g.map(sns.kdeplot, \"pearson\",\n",
    "      bw_adjust=.5, clip_on=False,\n",
    "      fill=True, alpha=1, linewidth=1.5)\n",
    "g.map(sns.kdeplot, \"pearson\", clip_on=False, color=\"w\", lw=2, bw_adjust=.5)\n",
    "\n",
    "# passing color=None to refline() uses the hue mapping\n",
    "g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n",
    "\n",
    "# Define and use a simple function to label the plot in axes coordinates\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.text(0, .2, label, fontweight=\"bold\", color=color,\n",
    "            ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "g.map(label, \"pearson\")\n",
    "\n",
    "# Set the subplots to overlap\n",
    "g.figure.subplots_adjust(hspace=-.3)\n",
    "\n",
    "# Remove axes details that don't play well with overlap\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[], ylabel=\"\")\n",
    "g.despine(bottom=True, left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a693d",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09dd801",
   "metadata": {},
   "source": [
    "## Check if the sequences are in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9bca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(human_sequences, memory_map=True, header=None, index_col=False, delimiter=\"\\t\")\n",
    "# keep only validation intervals \n",
    "validation_intervals= df[df[3]==\"valid\"]\n",
    "#validation_intervals = validation_intervals.head()\n",
    "# create list with interval\n",
    "interval_list = list()\n",
    "validation_intervals.apply(lambda row : interval_list.append(kipoiseq.Interval(row[0],row[1], row[2])), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary for search (can be improved! quite slow)\n",
    "human_validation_dict = {}\n",
    "for interval in interval_list: \n",
    "    sequence = one_hot_encode(fasta_extractor.extract(interval))\n",
    "    human_validation_dict[interval] = sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "import pickle\n",
    "\n",
    "enformer_dict_file = os.path.join(outputdir,'00_enformer_dict_seqs.h5')\n",
    "# Step 2\n",
    "with open(enformer_dict_file, 'wb') as config_dictionary_file:\n",
    "    pickle.dump(human_validation_dict, config_dictionary_file)\n",
    "    \n",
    "# -------- read -------\n",
    "with open(enformer_dict_file, 'rb') as config_dictionary_file:\n",
    "    config_dictionary = pickle.load(config_dictionary_file)\n",
    "\n",
    "print(config_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a408c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interval_from_sequence(sequence, human_validation_dict=human_validation_dict): \n",
    "    for interval, sequence in human_validation_dict.items():\n",
    "        if np.allclose(sequence,first_dataset_entry):\n",
    "            return(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a1d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create new dataset\n",
    "dataset_197k = []\n",
    "NEW_SEQUENCE_LENGTH = 196_608\n",
    "max_steps = 10\n",
    "\n",
    "for i, batch in tqdm(enumerate(human_dataset)):\n",
    "    batch_197k = {}\n",
    "    # 1 from the sequence 131k get the sequence 197k\n",
    "    interval_test = get_interval_from_sequence(batch[\"sequence\"])\n",
    "    sequence_197k = one_hot_encode(fasta_extractor.extract(interval_test.resize(NEW_SEQUENCE_LENGTH)))\n",
    "    batch_197k[\"sequence\"] = tf.Variable(sequence_197k[np.newaxis])\n",
    "    \n",
    "    # add same real targets\n",
    "    batch_197k[\"target\"] = batch[\"target\"]\n",
    "    dataset_197k.append(batch_197k)\n",
    "    if max_steps is not None and i > max_steps:\n",
    "        break\n",
    "\n",
    "        \n",
    "# ------ Save\n",
    "file = os.path.join(outputdir,'new_dataset_197k_valid.h5')\n",
    "# Step 2\n",
    "with open(file, 'wb') as config_dictionary_file:\n",
    "    pickle.dump(dataset_197k, config_dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eaeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file, 'rb') as config_dictionary_file:\n",
    "    dataset_197k = pickle.load(config_dictionary_file)\n",
    "\n",
    "for i, batch in enumerate(dataset_197k): \n",
    "    mybatch = batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1050dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try predictions\n",
    "def evaluate_model_all_sequences_mod(model, dataset_list, head, max_steps=None):\n",
    "    \n",
    "    metric = MetricDict({'PearsonR': PearsonR(reduce_axis=(0,1))})\n",
    "    print(\"Metric dictionary created\")\n",
    "    \n",
    "    def predict(x):\n",
    "        padded_sequence = pad_one_hot(np.squeeze(x.numpy(), axis=0), SEQUENCE_LENGHT)[np.newaxis]\n",
    "        predictions = model.predict_on_batch(padded_sequence)[head]\n",
    "        return tf.convert_to_tensor(predictions, dtype=tf.float32)\n",
    "    \n",
    "    i = 0 \n",
    "    for i, batch in enumerate(dataset_197k): \n",
    "        if max_steps is not None and i > max_steps:\n",
    "            break\n",
    "        i = i+1\n",
    "        prediction = predict(sequence_dict['sequence'])\n",
    "        metric.update_state(sequence_dict['target'], prediction)\n",
    "\n",
    "    return metric.result()\n",
    "\n",
    "\n",
    "\n",
    "metrics_human = evaluate_model_all_sequences(model,\n",
    "                               dataset= dataset_197k,\n",
    "                               head='human',\n",
    "                               max_steps=2)\n",
    "#print('')dataset_197k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894667e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right now it evaluates the whole model and \n",
    "metrics_human = evaluate_model_all_sequences(model,\n",
    "                               dataset=dataset_197k,\n",
    "                               head='human',\n",
    "                               max_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b0c94",
   "metadata": {},
   "source": [
    "## Create new TF record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a76edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update metadata\n",
    "metadata_human_197k = get_metadata(\"human\")\n",
    "metadata_human_197k['seq_length'] = NEW_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a61fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, head):\n",
    "    padded_sequence = pad_one_hot(x, SEQUENCE_LENGHT)[np.newaxis]\n",
    "    predictions = model.predict_on_batch(padded_sequence)[head]\n",
    "    return tf.convert_to_tensor(predictions, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f5488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test entry \n",
    "#interval_test = get_interval_from_sequence(first_dataset_entry)\n",
    "first_dataset_entry\n",
    "target_one = predict(first_dataset_entry, \"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean feature, encoded as False or True.\n",
    "n_observations = 10\n",
    "feature0 = np.random.choice([False, True], n_observations)\n",
    "#feature0 =  (first_dataset_entry)\n",
    "# Integer feature, random from 0 to 4.\n",
    "feature1 = np.random.randint(0, 5, n_observations)\n",
    "#feature1 = target_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd8ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f98d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dataset = tf.data.Dataset.from_tensor_slices((feature0, feature1, feature2, feature3))\n",
    "features_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dataset = tf.data.Dataset.from_tensor_slices((feature0, feature0))\n",
    "features_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaee577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the `tf.train.Example` observations to the file.\n",
    "tfrrecord_file = os.path.join(datadir, \"tfr/validation_196k.tfr\")\n",
    "n_observations = 1\n",
    "with tf.io.TFRecordWriter(tfrrecord_file) as writer:\n",
    "    for i in range(n_observations):\n",
    "        example = serialize_example(feature0[i].tobytes(), feature1[i].tobytes())\n",
    "        writer.write(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.train.Example.\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def serialize_example(feature0, feature1):\n",
    "\n",
    "    feature = {\n",
    "      'sequence': _bytes_feature(feature0),\n",
    "      'target': _bytes_feature(feature1),\n",
    "      }\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def deserialize(serialized_example, metadata):\n",
    "    \"\"\"Deserialize bytes stored in TFRecordFile.\"\"\"\n",
    "    feature_map = {\n",
    "          'sequence': tf.io.FixedLenFeature([], tf.string),\n",
    "          'target': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_example(serialized_example, feature_map)\n",
    "    sequence = tf.io.decode_raw(example['sequence'], tf.bool)\n",
    "    sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\n",
    "    sequence = tf.cast(sequence, tf.float32)\n",
    "\n",
    "    target = tf.io.decode_raw(example['target'], tf.float16)\n",
    "    target = tf.reshape(target,\n",
    "                          (metadata['target_length'], metadata['num_targets']))\n",
    "    target = tf.cast(target, tf.float32)\n",
    "\n",
    "    return {'sequence': sequence,\n",
    "              'target': target}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b5d19",
   "metadata": {},
   "source": [
    "## Test get_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(tfrrecord_file,\n",
    "                                        compression_type='ZLIB',\n",
    "                                        num_parallel_reads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ad6442",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(functools.partial(deserialize, metadata=metadata_human_197k),\n",
    "                            num_parallel_calls=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c678f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dataset.batch(1).prefetch(2)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9ada2ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4048325/1025821981.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sequence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "batch[\"sequence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7321a241",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'human_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4048325/3727846129.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmybatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'human_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for i, batch in tqdm(enumerate(human_dataset)):\n",
    "    print(i)\n",
    "    mybatch = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb8fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 1\n",
    "for i, batch in tqdm(enumerate(dataset)):\n",
    "    if max_steps is not None and i > max_steps:\n",
    "        break\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72485eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(organism, subset, num_threads=8):\n",
    "    \n",
    "    metadata = get_metadata(organism)\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n",
    "                                        compression_type='ZLIB',\n",
    "                                        num_parallel_reads=num_threads)\n",
    "    dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
    "                            num_parallel_calls=num_threads)\n",
    "    return dataset\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
