{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811f713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 12:25:50.310308: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-24 12:25:50.310326: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f0be12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sonnet as snt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "assert snt.__version__.startswith('2.0')\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18e383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get enformer source code\n",
    "#!wget -q https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer/attention_module.py\n",
    "#!wget -q https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer/enformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca009e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enformer\n",
    "datadir = \"../../../../data/FED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5055685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(organism):\n",
    "    targets_txt = f'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_{organism}.txt'\n",
    "    return pd.read_csv(targets_txt, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98401934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title `get_dataset(organism, subset, num_threads=8)`\n",
    "import glob\n",
    "import json\n",
    "import functools\n",
    "\n",
    "\n",
    "def organism_path(organism):\n",
    "    return os.path.join('gs://basenji_barnyard/data', organism)\n",
    "\n",
    "#\n",
    "def get_dataset(organism, subset, num_threads=8):\n",
    "    \n",
    "    metadata = get_metadata(organism)\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n",
    "                                        compression_type='ZLIB',\n",
    "                                        num_parallel_reads=num_threads)\n",
    "    dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
    "                            num_parallel_calls=num_threads)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_metadata(organism):\n",
    "  # Keys:\n",
    "  # num_targets, train_seqs, valid_seqs, test_seqs, seq_length,\n",
    "  # pool_width, crop_bp, target_length\n",
    "    path = os.path.join(organism_path(organism), 'statistics.json')\n",
    "    with tf.io.gfile.GFile(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def tfrecord_files(organism, subset):\n",
    "  # Sort the values by int(*).\n",
    "  return sorted(tf.io.gfile.glob(os.path.join(\n",
    "      organism_path(organism), 'tfrecords', f'{subset}-*.tfr'\n",
    "  )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\n",
    "\n",
    "\n",
    "def deserialize(serialized_example, metadata):\n",
    "    \"\"\"Deserialize bytes stored in TFRecordFile.\"\"\"\n",
    "    feature_map = {\n",
    "          'sequence': tf.io.FixedLenFeature([], tf.string),\n",
    "          'target': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_example(serialized_example, feature_map)\n",
    "    sequence = tf.io.decode_raw(example['sequence'], tf.bool)\n",
    "    sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\n",
    "    sequence = tf.cast(sequence, tf.float32)\n",
    "\n",
    "    target = tf.io.decode_raw(example['target'], tf.float16)\n",
    "    target = tf.reshape(target,\n",
    "                          (metadata['target_length'], metadata['num_targets']))\n",
    "    target = tf.cast(target, tf.float32)\n",
    "\n",
    "    return {'sequence': sequence,\n",
    "              'target': target}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f7e5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>genome</th>\n",
       "      <th>identifier</th>\n",
       "      <th>file</th>\n",
       "      <th>clip</th>\n",
       "      <th>scale</th>\n",
       "      <th>sum_stat</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCFF833POA</td>\n",
       "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>mean</td>\n",
       "      <td>DNASE:cerebellum male adult (27 years) and mal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCFF110QGM</td>\n",
       "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>mean</td>\n",
       "      <td>DNASE:frontal cortex male adult (27 years) and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCFF880MKD</td>\n",
       "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>mean</td>\n",
       "      <td>DNASE:chorion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCFF463ZLQ</td>\n",
       "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>mean</td>\n",
       "      <td>DNASE:Ishikawa treated with 0.02% dimethyl sul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCFF890OGQ</td>\n",
       "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>mean</td>\n",
       "      <td>DNASE:GM03348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  genome   identifier  \\\n",
       "0      0       0  ENCFF833POA   \n",
       "1      1       0  ENCFF110QGM   \n",
       "2      2       0  ENCFF880MKD   \n",
       "3      3       0  ENCFF463ZLQ   \n",
       "4      4       0  ENCFF890OGQ   \n",
       "\n",
       "                                                file  clip  scale sum_stat  \\\n",
       "0  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
       "1  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
       "2  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
       "3  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
       "4  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
       "\n",
       "                                         description  \n",
       "0  DNASE:cerebellum male adult (27 years) and mal...  \n",
       "1  DNASE:frontal cortex male adult (27 years) and...  \n",
       "2                                      DNASE:chorion  \n",
       "3  DNASE:Ishikawa treated with 0.02% dimethyl sul...  \n",
       "4                                      DNASE:GM03348  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targets_human = get_targets('human')\n",
    "df_targets_human.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79006475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 16:43:15.818309: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n",
      "2022-01-19 16:43:18.214703: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-01-19 16:43:18.214724: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-01-19 16:43:18.214753: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (luisasantus-HP-EliteDesk-800-G5-TWR): /proc/driver/nvidia/version does not exist\n",
      "2022-01-19 16:43:18.215317: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "human_dataset = get_dataset('human', 'train').batch(1).repeat()\n",
    "mouse_dataset = get_dataset('mouse', 'train').batch(1).repeat()\n",
    "human_mouse_dataset = tf.data.Dataset.zip((human_dataset, mouse_dataset)).prefetch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f00cb562",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(mouse_dataset)\n",
    "example = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "803c7e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8192.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((896*128)-131072)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "505be954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n",
      "{'sequence': (TensorShape([1, 131072, 4]), tf.float32), 'target': (TensorShape([1, 896, 5313]), tf.float32)}\n",
      "{'sequence': <tf.Tensor: shape=(1, 131072, 4), dtype=float32, numpy=\n",
      "array([[[0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.]]], dtype=float32)>, 'target': <tf.Tensor: shape=(1, 896, 5313), dtype=float32, numpy=\n",
      "array([[[0.10839844, 0.10760498, 0.04425049, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.10162354, 0.09332275, 0.0094986 , ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.10272217, 0.15600586, 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.07714844, 0.07678223, 0.03509521, ..., 0.        ,\n",
      "         0.01934814, 0.        ],\n",
      "        [0.07666016, 0.03826904, 0.05648804, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.08319092, 0.06051636, 0.02156067, ..., 0.        ,\n",
      "         0.        , 0.        ]]], dtype=float32)>}\n",
      "mouse\n",
      "{'sequence': (TensorShape([1, 131072, 4]), tf.float32), 'target': (TensorShape([1, 896, 1643]), tf.float32)}\n",
      "{'sequence': <tf.Tensor: shape=(1, 131072, 4), dtype=float32, numpy=\n",
      "array([[[0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        ...,\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.]]], dtype=float32)>, 'target': <tf.Tensor: shape=(1, 896, 1643), dtype=float32, numpy=\n",
      "array([[[0.00774384, 0.06262207, 0.0579834 , ..., 1.1318359 ,\n",
      "         0.        , 0.6035156 ],\n",
      "        [0.02897644, 0.04907227, 0.12213135, ..., 0.00302887,\n",
      "         1.9921875 , 0.09564209],\n",
      "        [0.05709839, 0.08172607, 0.07495117, ..., 0.9848633 ,\n",
      "         0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.0199585 , 0.0397644 , 0.01802063, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.01231384, 0.0163269 , 0.01808167, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.0050621 , 0.06262207, 0.01885986, ..., 0.        ,\n",
      "         0.        , 0.        ]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "it = iter(human_mouse_dataset)\n",
    "example = next(it)\n",
    "for i in range(len(example)):\n",
    "    print(['human', 'mouse'][i])\n",
    "    print({k: (v.shape, v.dtype) for k,v in example[i].items()})\n",
    "    print({k: (v) for k,v in example[i].items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dce1ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=get_dataset('human', 'valid').batch(1).prefetch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb5120bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_step_function(model, optimizer):\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(batch, head, optimizer_clip_norm_global=0.2):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(batch['sequence'], is_training=True)[head]\n",
    "            loss = tf.reduce_mean(\n",
    "              tf.keras.losses.poisson(batch['target'], outputs))\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply(gradients, model.trainable_variables)\n",
    "\n",
    "        return loss\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c943fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.Variable(0., trainable=False, name='learning_rate')\n",
    "optimizer = snt.optimizers.Adam(learning_rate=learning_rate)\n",
    "num_warmup_steps = 5000\n",
    "target_learning_rate = 0.0005\n",
    "\n",
    "model = enformer.Enformer(channels=1536 // 4,  # Use 4x fewer channels to train faster.\n",
    "                          num_heads=8,\n",
    "                          num_transformer_layers=11,\n",
    "                          pooling_type='max')\n",
    "\n",
    "train_step = create_step_function(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "964d9570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                               | 0/20 [00:00<?, ?it/s]2022-01-19 12:33:58.764247: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-01-19 12:34:22.120502: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 100663296 exceeds 10% of free system memory.\n",
      "2022-01-19 12:34:22.244483: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 100663296 exceeds 10% of free system memory.\n",
      "2022-01-19 12:34:22.417797: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 100663296 exceeds 10% of free system memory.\n",
      "2022-01-19 12:34:22.545658: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 100663296 exceeds 10% of free system memory.\n",
      "2022-01-19 12:34:22.635558: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 100663296 exceeds 10% of free system memory.\n",
      " 10%|████████████████▋                                                                                                                                                      | 2/20 [01:35<14:23, 47.96s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3532306/3464306438.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss_human\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_human\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss_mouse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_mouse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mouse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;31m# End of epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "steps_per_epoch = 20\n",
    "num_epochs = 5\n",
    "\n",
    "data_it = iter(human_mouse_dataset)\n",
    "global_step = 0\n",
    "for epoch_i in range(num_epochs):\n",
    "    for i in tqdm(range(steps_per_epoch)):\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step > 1:\n",
    "            learning_rate_frac = tf.math.minimum(\n",
    "              1.0, global_step / tf.math.maximum(1.0, num_warmup_steps))      \n",
    "            learning_rate.assign(target_learning_rate * learning_rate_frac)\n",
    "\n",
    "        batch_human, batch_mouse = next(data_it)\n",
    "\n",
    "        loss_human = train_step(batch=batch_human, head='human')\n",
    "        loss_mouse = train_step(batch=batch_mouse, head='mouse')\n",
    "\n",
    "  # End of epoch.\n",
    "    print('')\n",
    "    print('loss_human', loss_human.numpy(),\n",
    "        'loss_mouse', loss_mouse.numpy(),\n",
    "        'learning_rate', optimizer.learning_rate.numpy()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc31bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title `PearsonR` and `R2` metrics\n",
    "\n",
    "def _reduced_shape(shape, axis):\n",
    "    if axis is None:\n",
    "        return tf.TensorShape([])\n",
    "    return tf.TensorShape([d for i, d in enumerate(shape) if i not in axis])\n",
    "\n",
    "\n",
    "class CorrelationStats(tf.keras.metrics.Metric):\n",
    "    \"\"\"Contains shared code for PearsonR and R2.\"\"\"\n",
    "\n",
    "    def __init__(self, reduce_axis=None, name='pearsonr'):\n",
    "        \"\"\"Pearson correlation coefficient.\n",
    "\n",
    "        Args:\n",
    "          reduce_axis: Specifies over which axis to compute the correlation (say\n",
    "            (0, 1). If not specified, it will compute the correlation across the\n",
    "            whole tensor.\n",
    "          name: Metric name.\n",
    "        \"\"\"\n",
    "        super(CorrelationStats, self).__init__(name=name)\n",
    "        self._reduce_axis = reduce_axis\n",
    "        self._shape = None  # Specified in _initialize.\n",
    "\n",
    "    def _initialize(self, input_shape):\n",
    "        # Remaining dimensions after reducing over self._reduce_axis.\n",
    "        self._shape = _reduced_shape(input_shape, self._reduce_axis)\n",
    "\n",
    "        weight_kwargs = dict(shape=self._shape, initializer='zeros')\n",
    "        self._count = self.add_weight(name='count', **weight_kwargs)\n",
    "        self._product_sum = self.add_weight(name='product_sum', **weight_kwargs)\n",
    "        self._true_sum = self.add_weight(name='true_sum', **weight_kwargs)\n",
    "        self._true_squared_sum = self.add_weight(name='true_squared_sum',\n",
    "                                                 **weight_kwargs)\n",
    "        self._pred_sum = self.add_weight(name='pred_sum', **weight_kwargs)\n",
    "        self._pred_squared_sum = self.add_weight(name='pred_squared_sum',\n",
    "                                                 **weight_kwargs)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"Update the metric state.\n",
    "\n",
    "        Args:\n",
    "          y_true: Multi-dimensional float tensor [batch, ...] containing the ground\n",
    "            truth values.\n",
    "          y_pred: float tensor with the same shape as y_true containing predicted\n",
    "            values.\n",
    "          sample_weight: 1D tensor aligned with y_true batch dimension specifying\n",
    "            the weight of individual observations.\n",
    "        \"\"\"\n",
    "        if self._shape is None:\n",
    "          # Explicit initialization check.\n",
    "          self._initialize(y_true.shape)\n",
    "        y_true.shape.assert_is_compatible_with(y_pred.shape)\n",
    "        y_true = tf.cast(y_true, 'float32')\n",
    "        y_pred = tf.cast(y_pred, 'float32')\n",
    "\n",
    "        self._product_sum.assign_add(\n",
    "            tf.reduce_sum(y_true * y_pred, axis=self._reduce_axis))\n",
    "\n",
    "        self._true_sum.assign_add(\n",
    "            tf.reduce_sum(y_true, axis=self._reduce_axis))\n",
    "\n",
    "        self._true_squared_sum.assign_add(\n",
    "            tf.reduce_sum(tf.math.square(y_true), axis=self._reduce_axis))\n",
    "\n",
    "        self._pred_sum.assign_add(\n",
    "            tf.reduce_sum(y_pred, axis=self._reduce_axis))\n",
    "\n",
    "        self._pred_squared_sum.assign_add(\n",
    "            tf.reduce_sum(tf.math.square(y_pred), axis=self._reduce_axis))\n",
    "\n",
    "        self._count.assign_add(\n",
    "            tf.reduce_sum(tf.ones_like(y_true), axis=self._reduce_axis))\n",
    "\n",
    "    def result(self):\n",
    "        raise NotImplementedError('Must be implemented in subclasses.')\n",
    "\n",
    "    def reset_states(self):\n",
    "        if self._shape is not None:\n",
    "            tf.keras.backend.batch_set_value([(v, np.zeros(self._shape))\n",
    "                                        for v in self.variables])\n",
    "\n",
    "\n",
    "class PearsonR(CorrelationStats):\n",
    "    \"\"\"Pearson correlation coefficient.\n",
    "\n",
    "          Computed as:\n",
    "      ((x - x_avg) * (y - y_avg) / sqrt(Var[x] * Var[y])\n",
    "      \"\"\"\n",
    "\n",
    "    def __init__(self, reduce_axis=(0,), name='pearsonr'):\n",
    "        \"\"\"Pearson correlation coefficient.\n",
    "\n",
    "        Args:\n",
    "          reduce_axis: Specifies over which axis to compute the correlation.\n",
    "          name: Metric name.\n",
    "        \"\"\"\n",
    "        super(PearsonR, self).__init__(reduce_axis=reduce_axis,\n",
    "                                       name=name)\n",
    "\n",
    "    def result(self):\n",
    "        true_mean = self._true_sum / self._count\n",
    "        pred_mean = self._pred_sum / self._count\n",
    "\n",
    "        covariance = (self._product_sum\n",
    "                      - true_mean * self._pred_sum\n",
    "                      - pred_mean * self._true_sum\n",
    "                      + self._count * true_mean * pred_mean)\n",
    "\n",
    "        true_var = self._true_squared_sum - self._count * tf.math.square(true_mean)\n",
    "        pred_var = self._pred_squared_sum - self._count * tf.math.square(pred_mean)\n",
    "        tp_var = tf.math.sqrt(true_var) * tf.math.sqrt(pred_var)\n",
    "        correlation = covariance / tp_var\n",
    "\n",
    "        return correlation\n",
    "\n",
    "\n",
    "class R2(CorrelationStats):\n",
    "    \"\"\"R-squared  (fraction of explained variance).\"\"\"\n",
    "\n",
    "    def __init__(self, reduce_axis=None, name='R2'):\n",
    "        \"\"\"R-squared metric.\n",
    "\n",
    "        Args:\n",
    "          reduce_axis: Specifies over which axis to compute the correlation.\n",
    "          name: Metric name.\n",
    "        \"\"\"\n",
    "        super(R2, self).__init__(reduce_axis=reduce_axis,\n",
    "                                 name=name)\n",
    "\n",
    "    def result(self):\n",
    "        true_mean = self._true_sum / self._count\n",
    "        total = self._true_squared_sum - self._count * tf.math.square(true_mean)\n",
    "        residuals = (self._pred_squared_sum - 2 * self._product_sum\n",
    "                     + self._true_squared_sum)\n",
    "\n",
    "        return tf.ones_like(residuals) - residuals / total\n",
    "\n",
    "\n",
    "class MetricDict:\n",
    "    def __init__(self, metrics):\n",
    "        self._metrics = metrics\n",
    "\n",
    "    def update_state(self, y_true, y_pred):\n",
    "        for k, metric in self._metrics.items():\n",
    "            metric.update_state(y_true, y_pred)\n",
    "\n",
    "    def result(self):\n",
    "        return {k: metric.result() for k, metric in self._metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f84e3f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, head, max_steps=None):\n",
    "    metric = MetricDict({'PearsonR': PearsonR(reduce_axis=(0,1))})\n",
    "    @tf.function\n",
    "    def predict(x):\n",
    "        return model(x, is_training=False)[head]\n",
    "\n",
    "    for i, batch in tqdm(enumerate(dataset)):\n",
    "        if max_steps is not None and i > max_steps:\n",
    "            break\n",
    "        metric.update_state(batch['target'], predict(batch['sequence']))\n",
    "    return metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "09c668fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [02:48,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'PearsonR': 0.0005675854}\n"
     ]
    }
   ],
   "source": [
    "metrics_human = evaluate_model(model,\n",
    "                               dataset=get_dataset('human', 'valid').batch(1).prefetch(2),\n",
    "                               head='human',\n",
    "                               max_steps=100)\n",
    "print('')\n",
    "print({k: v.numpy().mean() for k, v in metrics_human.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5da2236a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Enformer(channels=384, pooling_type='max')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58c07c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['PearsonR'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_human.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88350aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_mouse = evaluate_model(model,\n",
    "                               dataset=get_dataset('mouse', 'valid').batch(1).prefetch(2),\n",
    "                               head='mouse',\n",
    "                               max_steps=100)\n",
    "print('')\n",
    "print({k: v.numpy().mean() for k, v in metrics_mouse.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4721313",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1852686607.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_3807743/1852686607.py\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    super().__init__(name=name)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from typing import Any, Callable, Dict, Optional, Text, Union, Iterable\n",
    "import attention_module\n",
    "import numpy as np\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "\n",
    "SEQUENCE_LENGTH = 196_608\n",
    "BIN_SIZE = 128\n",
    "TARGET_LENGTH = 896\n",
    "\n",
    "\n",
    "class Enformer(snt.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "               channels: int = 1536,\n",
    "               num_transformer_layers: int = 11,\n",
    "               num_heads: int = 8,\n",
    "               pooling_type: str = 'attention',\n",
    "               name: str = 'enformer'):\n",
    "\n",
    "    super().__init__(name=name)\n",
    "    # pylint: disable=g-complex-comprehension,g-long-lambda,cell-var-from-loop\n",
    "    heads_channels = {'human': 5313, 'mouse': 1643}\n",
    "    dropout_rate = 0.4\n",
    "    assert channels % num_heads == 0, ('channels needs to be divisible '\n",
    "                                       f'by {num_heads}')\n",
    "    whole_attention_kwargs = {\n",
    "        'attention_dropout_rate': 0.05,\n",
    "        'initializer': None,\n",
    "        'key_size': 64,\n",
    "        'num_heads': num_heads,\n",
    "        'num_relative_position_features': channels // num_heads,\n",
    "        'positional_dropout_rate': 0.01,\n",
    "        'relative_position_functions': [\n",
    "            'positional_features_exponential',\n",
    "            'positional_features_central_mask',\n",
    "            'positional_features_gamma'\n",
    "        ],\n",
    "        'relative_positions': True,\n",
    "        'scaling': True,\n",
    "        'value_size': channels // num_heads,\n",
    "        'zero_initialize': True\n",
    "    }\n",
    "\n",
    "    trunk_name_scope = tf.name_scope('trunk')\n",
    "    trunk_name_scope.__enter__()\n",
    "    # lambda is used in Sequential to construct the module under tf.name_scope.\n",
    "    def conv_block(filters, width=1, w_init=None, name='conv_block', **kwargs):\n",
    "        return Sequential(lambda: [\n",
    "          snt.BatchNorm(create_scale=True,\n",
    "                        create_offset=True,\n",
    "                        decay_rate=0.9,\n",
    "                        scale_init=snt.initializers.Ones()),\n",
    "          gelu,\n",
    "          snt.Conv1D(filters, width, w_init=w_init, **kwargs)\n",
    "      ], name=name)\n",
    "\n",
    "    stem = Sequential(lambda: [\n",
    "        snt.Conv1D(channels // 2, 15),\n",
    "        Residual(conv_block(channels // 2, 1, name='pointwise_conv_block')),\n",
    "        pooling_module(pooling_type, pool_size=2),\n",
    "    ], name='stem')\n",
    "\n",
    "    filter_list = exponential_linspace_int(start=channels // 2, end=channels,\n",
    "                                           num=6, divisible_by=128)\n",
    "    conv_tower = Sequential(lambda: [\n",
    "        Sequential(lambda: [\n",
    "            conv_block(num_filters, 5),\n",
    "            Residual(conv_block(num_filters, 1, name='pointwise_conv_block')),\n",
    "            pooling_module(pooling_type, pool_size=2),\n",
    "            ],\n",
    "                   name=f'conv_tower_block_{i}')\n",
    "        for i, num_filters in enumerate(filter_list)], name='conv_tower')\n",
    "\n",
    "    # Transformer.\n",
    "    def transformer_mlp():\n",
    "        return Sequential(lambda: [\n",
    "          snt.LayerNorm(axis=-1, create_scale=True, create_offset=True),\n",
    "          snt.Linear(channels * 2),\n",
    "          snt.Dropout(dropout_rate),\n",
    "          tf.nn.relu,\n",
    "          snt.Linear(channels),\n",
    "          snt.Dropout(dropout_rate)], name='mlp')\n",
    "\n",
    "    transformer = Sequential(lambda: [\n",
    "        Sequential(lambda: [\n",
    "            Residual(Sequential(lambda: [\n",
    "                snt.LayerNorm(axis=-1,\n",
    "                              create_scale=True, create_offset=True,\n",
    "                              scale_init=snt.initializers.Ones()),\n",
    "                attention_module.MultiheadAttention(**whole_attention_kwargs,\n",
    "                                                    name=f'attention_{i}'),\n",
    "                snt.Dropout(dropout_rate)], name='mha')),\n",
    "            Residual(transformer_mlp())], name=f'transformer_block_{i}')\n",
    "        for i in range(num_transformer_layers)], name='transformer')\n",
    "\n",
    "    crop_final = TargetLengthCrop1D(TARGET_LENGTH, name='target_input')\n",
    "\n",
    "    final_pointwise = Sequential(lambda: [\n",
    "        conv_block(channels * 2, 1),\n",
    "        snt.Dropout(dropout_rate / 8),\n",
    "        gelu], name='final_pointwise')\n",
    "\n",
    "    self._trunk = Sequential([stem,\n",
    "                              conv_tower,\n",
    "                              transformer,\n",
    "                              crop_final,\n",
    "                              final_pointwise],\n",
    "                             name='trunk')\n",
    "    trunk_name_scope.__exit__(None, None, None)\n",
    "\n",
    "    with tf.name_scope('heads'):\n",
    "        self._heads = {\n",
    "          head: Sequential(\n",
    "              lambda: [snt.Linear(num_channels), tf.nn.softplus],\n",
    "              name=f'head_{head}')\n",
    "          for head, num_channels in heads_channels.items()\n",
    "      }\n",
    "    # pylint: enable=g-complex-comprehension,g-long-lambda,cell-var-from-loop\n",
    "\n",
    "    @property\n",
    "    def trunk(self):\n",
    "        return self._trunk\n",
    "\n",
    "    @property\n",
    "    def heads(self):\n",
    "        return self._heads\n",
    "\n",
    "    def __call__(self, inputs: tf.Tensor,\n",
    "               is_training: bool) -> Dict[str, tf.Tensor]:\n",
    "        trunk_embedding = self.trunk(inputs, is_training=is_training)\n",
    "        return {\n",
    "            head: head_module(trunk_embedding, is_training=is_training)\n",
    "            for head, head_module in self.heads.items()\n",
    "        }\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "      tf.TensorSpec([None, SEQUENCE_LENGTH, 4], tf.float32)])\n",
    "    def predict_on_batch(self, x):\n",
    "    \"\"\"Method for SavedModel.\"\"\"\n",
    "        return self(x, is_training=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
