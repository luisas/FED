{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da5f651",
   "metadata": {},
   "source": [
    "# Create validation dataset 197k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23917398",
   "metadata": {},
   "source": [
    "### import modules and load local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d49b248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import importlib\n",
    "import kipoiseq\n",
    "from kipoiseq import Interval\n",
    "import pyfaidx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b61d7fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Faidx(\"../../../../../data/FED/hg38.fa\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir = \"../../../../../data/FED\"\n",
    "#datadir = sys.argv[1]\n",
    "outputdir = os.path.join(datadir, \"hd5\")\n",
    "fasta_file = os.path.join(datadir, \"hg38.fa\")\n",
    "human_sequences = os.path.join(datadir, \"data_human_sequences.bed\")\n",
    "pyfaidx.Faidx(fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6576006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Mapping\n",
    "import json\n",
    "import functools\n",
    "\n",
    "\n",
    "def _reduced_shape(shape, axis):\n",
    "    if axis is None:\n",
    "        return tf.TensorShape([])\n",
    "    return tf.TensorShape([d for i, d in enumerate(shape) if i not in axis])\n",
    "\n",
    "def organism_path(organism, prefix):\n",
    "    return os.path.join(prefix, organism)\n",
    "\n",
    "#\n",
    "def get_dataset(organism, subset, prefix, num_threads=8):\n",
    "\n",
    "    metadata = get_metadata(organism, prefix)\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset, prefix = prefix),\n",
    "                                        compression_type='ZLIB',\n",
    "                                        num_parallel_reads=num_threads)\n",
    "\n",
    "    dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
    "                            num_parallel_calls=num_threads)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_metadata(organism, prefix):\n",
    "  # Keys:\n",
    "  # num_targets, train_seqs, valid_seqs, test_seqs, seq_length,\n",
    "  # pool_width, crop_bp, target_length\n",
    "    path = os.path.join(organism_path(organism, prefix), 'statistics.json')\n",
    "    with tf.io.gfile.GFile(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def tfrecord_files(organism, subset, prefix):\n",
    "  # Sort the values by int(*).\n",
    "  return sorted(tf.io.gfile.glob(os.path.join(\n",
    "      organism_path(organism, prefix), 'tfrecords', f'{subset}-*.tfr'\n",
    "  )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\n",
    "\n",
    "\n",
    "def deserialize(serialized_example, metadata):\n",
    "    \"\"\"Deserialize bytes stored in TFRecordFile.\"\"\"\n",
    "    feature_map = {\n",
    "          'sequence': tf.io.FixedLenFeature([], tf.string),\n",
    "          'target': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_example(serialized_example, feature_map)\n",
    "    sequence = tf.io.decode_raw(example['sequence'], tf.bool)\n",
    "    sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\n",
    "    sequence = tf.cast(sequence, tf.float32)\n",
    "\n",
    "    target = tf.io.decode_raw(example['target'], tf.float16)\n",
    "    target = tf.reshape(target,\n",
    "                          (metadata['target_length'], metadata['num_targets']))\n",
    "    target = tf.cast(target, tf.float32)\n",
    "\n",
    "    return {'sequence': sequence,\n",
    "              'target': target}\n",
    "\n",
    "\n",
    "\n",
    "class FastaStringExtractor:\n",
    "\n",
    "    def __init__(self, fasta_file):\n",
    "        self.fasta = pyfaidx.Fasta(fasta_file)\n",
    "        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n",
    "\n",
    "    def extract(self, interval: Interval, **kwargs) -> str:\n",
    "        # Truncate interval if it extends beyond the chromosome lengths.\n",
    "        chromosome_length = self._chromosome_sizes[interval.chrom]\n",
    "        trimmed_interval = Interval(interval.chrom,\n",
    "                                    max(interval.start, 0),\n",
    "                                    min(interval.end, chromosome_length),\n",
    "                                    )\n",
    "        # pyfaidx wants a 1-based interval\n",
    "        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n",
    "                                          trimmed_interval.start + 1,\n",
    "                                          trimmed_interval.stop).seq).upper()\n",
    "        # Fill truncated values with N's.\n",
    "        pad_upstream = 'N' * max(-interval.start, 0)\n",
    "        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n",
    "        return pad_upstream + sequence + pad_downstream\n",
    "\n",
    "    def close(self):\n",
    "        return self.fasta.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8568b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_extractor = FastaStringExtractor(fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261b9985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary for search (can be improved! quite slow)\n",
    "#human_validation_dict = {}\n",
    "#for interval in interval_list: \n",
    "#    sequence = one_hot_encode(fasta_extractor.extract(interval))\n",
    "#    human_validation_dict[interval] = sequence\n",
    "    \n",
    "# -------- save\n",
    "enformer_dict_file = os.path.join(outputdir,'00_enformer_dict_seqs.h5')\n",
    "#with open(enformer_dict_file, 'wb') as config_dictionary_file:\n",
    "#    pickle.dump(human_validation_dict, config_dictionary_file)\n",
    "    \n",
    "# -------- read \n",
    "with open(enformer_dict_file, 'rb') as config_dictionary_file:\n",
    "    human_validation_dict = pickle.load(config_dictionary_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8a8699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences in dictionary\n",
      "2213\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sequences in dictionary\")\n",
    "print(len(human_validation_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ca0826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interval_from_sequence(sequence, human_validation_dict=human_validation_dict): \n",
    "    for interval, ref_sequence in human_validation_dict.items():\n",
    "        if np.allclose(sequence,ref_sequence):\n",
    "            return(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e6a52bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with older sequences \n",
    "human_dataset = get_dataset('human', 'valid', \"/home/luisasantus/Desktop/crg_cluster/data/FED/basenji/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a63984ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "200%100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "818785c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n"
     ]
    }
   ],
   "source": [
    "num_training_examples = 0\n",
    "num_validation_examples = 0\n",
    "\n",
    "for example in human_dataset:\n",
    "    num_training_examples += 1\n",
    "    if(num_training_examples % 100 == 0 ):\n",
    "        print(num_training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3aea4a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2214"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_training_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "623a1c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'one_hot_encode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_118454/3343539.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# 1 from the sequence 131k get the sequence 197k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0minterval_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_interval_from_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sequence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msequence_197k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfasta_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterval_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNEW_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mbatch_197k\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sequence\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_197k\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'one_hot_encode' is not defined"
     ]
    }
   ],
   "source": [
    "## Create new dataset\n",
    "dataset_197k = []\n",
    "NEW_SEQUENCE_LENGTH = 196_608\n",
    "max_steps = 2\n",
    "\n",
    "for i, batch in tqdm(enumerate(human_dataset)):\n",
    "    batch_197k = {}\n",
    "    # 1 from the sequence 131k get the sequence 197k\n",
    "    interval_test = get_interval_from_sequence(batch[\"sequence\"])\n",
    "    sequence_197k = one_hot_encode(fasta_extractor.extract(interval_test.resize(NEW_SEQUENCE_LENGTH)))\n",
    "    batch_197k[\"sequence\"] = tf.constant(sequence_197k[np.newaxis])\n",
    "    \n",
    "    # add same real targets\n",
    "    batch_197k[\"target\"] = batch[\"target\"]\n",
    "    dataset_197k.append(batch_197k)\n",
    "    if max_steps is not None and i > max_steps:\n",
    "        break\n",
    "\n",
    "file = os.path.join(outputdir,'new_dataset_197k_valid.h5')\n",
    "\n",
    "# Step 2\n",
    "with open(file, 'wb') as config_dictionary_file:\n",
    "    pickle.dump(dataset_197k, config_dictionary_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
