{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed624904",
   "metadata": {},
   "source": [
    "# Enformer human validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafbf0ab",
   "metadata": {},
   "source": [
    "### Load  pre-trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba173e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import pyfaidx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4ac79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-02 11:25:39.228451: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-02 11:25:39.228469: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_107042/786036427.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import kipoiseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkipoiseq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/kipoiseq/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataclasses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextractors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/kipoiseq/dataloaders/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msplicing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mprotein\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/kipoiseq/dataloaders/sequence.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkipoi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenomicRanges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkipoi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkipoi_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkipoi_conda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdependencies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/kipoi/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# available modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkipoi_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;31m# backward compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkipoi_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m \u001b[0;31m# backward compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/kipoi/pipeline.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkipoi_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkipoi\u001b[0m  \u001b[0;31m# for .config module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy_collate_concat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# import h5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/kipoi/data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrelated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkipoi\u001b[0m  \u001b[0;31m# for .config module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkipoi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoaderDescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRemoteFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_default_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoaderImport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/related/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m from .fields import (\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mBooleanField\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mChildField\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/related/fields.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# -*- coding: utf-8 -*-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdecimal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecimal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParseResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNOTHING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/future/moves/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mimport_top_level_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/future/standard_library/__init__.py\u001b[0m in \u001b[0;36mimport_top_level_modules\u001b[0;34m()\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimport_top_level_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mexclude_local_folder_imports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mTOP_LEVEL_MODULES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTOP_LEVEL_MODULES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/future/standard_library/__init__.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m#             pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m                 \u001b[0;31m# There's a problem importing the system module. E.g. the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/crg_cluster/projects/FED/src/01_enformer/test.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menformer_dict_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconfig_dictionary_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mhuman_validation_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dictionary_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----------------------------------------------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/kipoiseq/dataclasses.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m                 self.strand == obj.strand)\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchrom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import kipoiseq\n",
    "from kipoiseq import Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f7cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib \n",
    "spec_utils = importlib.util.spec_from_file_location(\"enformer\", os.path.join(os.getcwd() ,\"utils/utils.py\"))\n",
    "utils = importlib.util.module_from_spec(spec_utils)\n",
    "spec_utils.loader.exec_module(utils)\n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb9edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_utils.loader.exec_module(utils)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b6254a",
   "metadata": {},
   "source": [
    "### Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0900f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'https://tfhub.dev/deepmind/enformer/1'\n",
    "datadir = \"../../../../data/FED\"\n",
    "outputdir = os.path.join(datadir, \"hd5\")\n",
    "fasta_file = os.path.join(datadir, \"hg38.fa\")\n",
    "human_sequences = os.path.join(datadir, \"data_human_sequences.bed\")\n",
    "pyfaidx.Faidx(fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91386fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.Enformer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f9d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import enformer.py as module\n",
    "spec = importlib.util.spec_from_file_location(\"enformer\", os.path.join(os.getcwd() ,\"utils/enformer.py\"))\n",
    "enformer = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(enformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c542b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "    from utils.enformer import * \n",
    "    from utils.utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e380ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_extractor = utils.FastaStringExtractor(fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7eb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e9cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous validation dictionary\n",
    "dataset_197k_file = os.path.join(outputdir,'test_197k_valid.h5')\n",
    "\n",
    "with open(dataset_197k_file, 'rb') as config_dictionary_file:\n",
    "    dataset_197k = pickle.load(config_dictionary_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d206fba",
   "metadata": {},
   "source": [
    "# Evaluate per track "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4028ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGHT = 393_216\n",
    "## pad the sequence with Ns (anyways ignored by the model)\n",
    "def pad_one_hot(sequence_one_hot, NEW_SIZE):\n",
    "    ADD_ENDS = int((NEW_SIZE - sequence_one_hot.shape[0])/2)\n",
    "    pad_zero = np.tile(np.array([0., 0., 0., 0.]), (ADD_ENDS, 1))\n",
    "    padded_left = np.append(pad_zero,sequence_one_hot, axis=0)\n",
    "    pad_sequence = np.append(padded_left,pad_zero, axis=0)\n",
    "    return(pad_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efcd040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_all_sequences_mod(model, dataset, head, max_steps=None):\n",
    "    \n",
    "    metric = MetricDict({'PearsonR': PearsonR(reduce_axis=(0,1))})\n",
    "    \n",
    "    # Given a tensor with a one-encoded sequence, predicts head tracks\n",
    "    def predict(x):\n",
    "        padded_sequence = pad_one_hot(np.squeeze(x.numpy(), axis=0), SEQUENCE_LENGHT)[np.newaxis]\n",
    "        predictions = model.predict_on_batch(padded_sequence)[head]\n",
    "        return tf.convert_to_tensor(predictions, dtype=tf.float32)\n",
    "    \n",
    "    i = 0 \n",
    "    for i, batch in enumerate(dataset_197k): \n",
    "        if max_steps is not None and i > max_steps:\n",
    "            break\n",
    "        i = i+1\n",
    "        prediction = predict(batch['sequence'])\n",
    "        metric.update_state(batch['target'], prediction)\n",
    "\n",
    "    return metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a9aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "dataset_197k_evaluation = copy.deepcopy(dataset_197k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6180e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_all_sequences_mod(model, dataset, head, dataset_197k_evaluation, max_steps=None):\n",
    "    \n",
    "    metric = MetricDict({'PearsonR': PearsonR(reduce_axis=(0,1))})\n",
    "    \n",
    "    # Given a tensor with a one-encoded sequence, predicts head tracks\n",
    "    def predict(x):\n",
    "        padded_sequence = pad_one_hot(np.squeeze(x.numpy(), axis=0), SEQUENCE_LENGHT)[np.newaxis]\n",
    "        predictions = model.predict_on_batch(padded_sequence)[head]\n",
    "        return tf.convert_to_tensor(predictions, dtype=tf.float32)\n",
    "    \n",
    "    for i, batch in enumerate(dataset_197k): \n",
    "        if max_steps is not None and i > max_steps:\n",
    "            break\n",
    "        \n",
    "        prediction = predict(batch['sequence'])\n",
    "        metric.update_state(batch['target'], prediction)\n",
    "        \n",
    "        # Compute it on a sequence basis\n",
    "        metric_seq = MetricDict({'PearsonR': PearsonR(reduce_axis=(0,1))})\n",
    "        metric_seq.update_state(batch['target'], prediction)\n",
    "        dataset_197k_evaluation[i][\"PearsonR\"] = metric.result()[\"PearsonR\"].numpy()\n",
    "        print(dataset_197k_evaluation[i][\"PearsonR\"])\n",
    "        i = i+1\n",
    "    return list([metric.result(), dataset_197k_evaluation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_human = evaluate_model_all_sequences_mod(model,\n",
    "                               dataset=dataset_197k[0:1],\n",
    "                               head='human',\n",
    "                               dataset_197k_evaluation = dataset_197k_evaluation,\n",
    "                               max_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3363a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_197k_evaluation[0][\"PearsonR\"] = dataset_197k_evaluation[1][\"PearsonR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a978247",
   "metadata": {},
   "source": [
    "# Plot sequences summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecebfd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_assays = suppl_df[suppl_df[\"organism\"] == \"human\"][\"assay_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c586822",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(suppl_df[suppl_df[\"organism\"] == \"human\"].groupby(\"assay_type\").count()[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23504148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_evaluation_df(i,dataset_197k_evaluation, ordered_assays ):\n",
    "    # Create dataframe for plotting\n",
    "    df = pd.DataFrame()\n",
    "    # Add sequence\n",
    "    df[\"sequence\"] = np.repeat(1,len(ordered_assays))\n",
    "    # Add assay\n",
    "    df[\"assay\"] = ordered_assays\n",
    "    # Add pearson values \n",
    "    df[\"pearson\"] = (dataset_197k_evaluation[i][\"PearsonR\"])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "for i in range(len(dataset_197k_evaluation)):\n",
    "    df = get_sequence_evaluation_df(i,dataset_197k_evaluation, ordered_assays)\n",
    "    print(i)\n",
    "    final_df = pd.concat([final_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b46571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"ticks\", palette=\"pastel\")\n",
    "\n",
    "# Load the example tips dataset\n",
    "\n",
    "# Draw a nested boxplot to show bills by day and time\n",
    "sns.boxplot(x=\"assay\", y=\"pearson\",  palette=\"mako\",\n",
    "            data=final_df)\n",
    "\n",
    "sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168c3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ecc12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8d2f5d7",
   "metadata": {},
   "source": [
    "### Check tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b5360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download targets from Basenji2 dataset \n",
    "# Cite: Kelley et al Cross-species regulatory sequence activity prediction. PLoS Comput. Biol. 16, e1008050 (2020).\n",
    "targets_txt = 'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_human.txt'\n",
    "df_targets = pd.read_csv(targets_txt, sep='\\t')\n",
    "df_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fcffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppl = pd.ExcelFile(os.path.join(datadir, \"enformer_suppl.xlsx\"))\n",
    "print(suppl.sheet_names)\n",
    "suppl_human = suppl.parse(suppl.sheet_names[1])\n",
    "suppl_mouse = suppl.parse(suppl.sheet_names[2])\n",
    "suppl_human[\"organism\"] = \"human\"\n",
    "suppl_mouse[\"organism\"] = \"mouse\"\n",
    "frames = [suppl_human, suppl_mouse]\n",
    "suppl_df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c5b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f917fd13",
   "metadata": {},
   "source": [
    "### compute score (how well predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21187f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO \n",
    "#1 - retrieve the 197k sequence instead o 131k \n",
    "prefix = \"gs://basenji_barnyard/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a615e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dataset = get_dataset('human', 'valid', prefix).batch(1).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06daf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(human_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_all_sequences(model, dataset, head, max_steps=None):\n",
    "    \n",
    "    metric = MetricDict({'PearsonR': PearsonR(reduce_axis=(0,1))})\n",
    "    print(\"Metric dictionary created\")\n",
    "    \n",
    "    def predict(x):\n",
    "        print(\"Beginning prediction\")\n",
    "        padded_sequence = pad_one_hot(np.squeeze(x.numpy(), axis=0), SEQUENCE_LENGHT)[np.newaxis]\n",
    "        predictions = model.predict_on_batch(padded_sequence)[head]\n",
    "        return tf.convert_to_tensor(predictions, dtype=tf.float32)\n",
    "    print(\"Predict funciton loaded\")\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(dataset)):\n",
    "        if max_steps is not None and i > max_steps:\n",
    "            break\n",
    "        prediction = predict(batch['sequence'])\n",
    "        metric.update_state(batch['target'], prediction)\n",
    "\n",
    "    return metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ddb14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on first ten \n",
    "# Right now it evaluates the whole model and \n",
    "metrics_human = evaluate_model_all_sequences(model,\n",
    "                               dataset=get_dataset('human', 'valid').batch(1).prefetch(2),\n",
    "                               head='human',\n",
    "                               max_steps=2)\n",
    "#print('')dataset_197k\n",
    "#print({k: v.numpy().mean() for k, v in metrics_human.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b4c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_all_sequences(model, sequence_dict, head, max_steps=None):\n",
    "    \n",
    "    metric = MetricDict({'PearsonR': PearsonR(reduce_axis=(0,1))})\n",
    "    print(\"Metric dictionary created\")\n",
    "    \n",
    "    def predict(x):\n",
    "        print(\"Beginning prediction\")\n",
    "        padded_sequence = pad_one_hot(np.squeeze(x.numpy(), axis=0), SEQUENCE_LENGHT)[np.newaxis]\n",
    "        predictions = model.predict_on_batch(padded_sequence)[head]\n",
    "        return tf.convert_to_tensor(predictions, dtype=tf.float32)\n",
    "    print(\"Predict funciton loaded\")\n",
    "    \n",
    "    i = 0 \n",
    "    for keys in sequence_dict.keys():\n",
    "        if max_steps is not None and i > max_steps:\n",
    "            break\n",
    "        i = i+1\n",
    "        prediction = predict(sequence_dict['sequence'])\n",
    "        metric.update_state(sequence_dict['target'], prediction)\n",
    "\n",
    "    return metric.result()\n",
    "\n",
    "\n",
    "\n",
    "metrics_human = evaluate_model_all_sequences(model,\n",
    "                               dataset= dataset_197k,\n",
    "                               head='human',\n",
    "                               max_steps=2)\n",
    "#print('')dataset_197k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd0b39",
   "metadata": {},
   "source": [
    "### PLOT: Distributions of pearson correlation coefficients per assay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming the values are in order of assay (TODO check) \n",
    "assay_list = list(suppl_df[\"assay_type\"])\n",
    "pearson_per_assay = list(metrics_human[0][\"PearsonR\"].numpy())\n",
    "data_tuples = list(zip(assay_list,pearson_per_assay))\n",
    "df_pearson_assay = pd.DataFrame(data_tuples, columns=['assay','pearson'])\n",
    "df_pearson_assay[\"pearson\"]\n",
    "df = df_pearson_assay\n",
    "df = df.astype({\"assay\": str, \"pearson\": float})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"assay\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f532139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "\n",
    "sns.set_theme(style=\"ticks\", palette=\"pastel\")\n",
    "\n",
    "# Load the example tips dataset\n",
    "\n",
    "# Draw a nested boxplot to show bills by day and time\n",
    "sns.violinplot(x=\"assay\", y=\"pearson\",  palette=\"mako\", linewidth=1.5,\n",
    "            data=df)\n",
    "sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ad2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Initialize \n",
    "g = sns.FacetGrid(df, row=\"assay\", hue=\"assay\", aspect=15, height=1, palette=\"mako\")\n",
    "\n",
    "# Draw the densities in a few steps\n",
    "g.map(sns.kdeplot, \"pearson\",\n",
    "      bw_adjust=.5, clip_on=False,\n",
    "      fill=True, alpha=1, linewidth=1.5)\n",
    "g.map(sns.kdeplot, \"pearson\", clip_on=False, color=\"w\", lw=2, bw_adjust=.5)\n",
    "\n",
    "# passing color=None to refline() uses the hue mapping\n",
    "g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n",
    "\n",
    "# Define and use a simple function to label the plot in axes coordinates\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.text(0, .2, label, fontweight=\"bold\", color=color,\n",
    "            ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "g.map(label, \"pearson\")\n",
    "\n",
    "# Set the subplots to overlap\n",
    "g.figure.subplots_adjust(hspace=-.3)\n",
    "\n",
    "# Remove axes details that don't play well with overlap\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[], ylabel=\"\")\n",
    "g.despine(bottom=True, left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a693d",
   "metadata": {},
   "source": [
    "# OLD (do not delete) - prepare  dictionary intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09dd801",
   "metadata": {},
   "source": [
    "## Check if the sequences are in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9bca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(human_sequences, memory_map=True, header=None, index_col=False, delimiter=\"\\t\")\n",
    "# keep only validation intervals \n",
    "validation_intervals= df[df[3]==\"valid\"]\n",
    "#validation_intervals = validation_intervals.head()\n",
    "# create list with interval\n",
    "interval_list = list()\n",
    "validation_intervals.apply(lambda row : interval_list.append(kipoiseq.Interval(row[0],row[1], row[2])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary for search (can be improved! quite slow)\n",
    "human_validation_dict = {}\n",
    "for interval in interval_list: \n",
    "    sequence = one_hot_encode(fasta_extractor.extract(interval))\n",
    "    human_validation_dict[interval] = sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "import pickle\n",
    "\n",
    "enformer_dict_file = os.path.join(outputdir,'00_enformer_dict_seqs.h5')\n",
    "# Step 2\n",
    "with open(enformer_dict_file, 'wb') as config_dictionary_file:\n",
    "    pickle.dump(human_validation_dict, config_dictionary_file)\n",
    "    \n",
    "# -------- read -------\n",
    "with open(enformer_dict_file, 'rb') as config_dictionary_file:\n",
    "    config_dictionary = pickle.load(config_dictionary_file)\n",
    "\n",
    "print(config_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b376d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a61886",
   "metadata": {},
   "source": [
    "## Convert Human dataset into list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8d734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dataset_list = []\n",
    "for i, batch in tqdm(enumerate(human_dataset)):\n",
    "    human_dataset_list.append(batch)\n",
    "\n",
    "file = os.path.join(outputdir,'human_dataset.h5')\n",
    "\n",
    "# Step 2\n",
    "with open(file, 'wb') as config_dictionary_file:\n",
    "    pickle.dump(human_dataset_list, config_dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b122d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2548d022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
