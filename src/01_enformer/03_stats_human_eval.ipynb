{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed624904",
   "metadata": {},
   "source": [
    "# Enformer human validation (smaller dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e5383c",
   "metadata": {},
   "source": [
    "## Evaluate sequence-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1bf277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17c7172",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../../../../data/FED\"\n",
    "outputdir = os.path.join(datadir, \"hd5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc021646",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(outputdir,'dataset_197k_evaluation_50.h5')\n",
    "with open(file, 'rb') as config_dictionary_file:\n",
    "    dataset_197k_evaluation = pickle.load(config_dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(outputdir,'summarized_metrics.h5')\n",
    "with open(file, 'rb') as config_dictionary_file:\n",
    "    summarized_metrics = pickle.load(config_dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc991fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download targets from Basenji2 dataset \n",
    "# Cite: Kelley et al Cross-species regulatory sequence activity prediction. PLoS Comput. Biol. 16, e1008050 (2020).\n",
    "targets_txt = 'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_human.txt'\n",
    "df_targets = pd.read_csv(targets_txt, sep='\\t')\n",
    "df_targets\n",
    "\n",
    "suppl = pd.ExcelFile(os.path.join(datadir, \"enformer_suppl.xlsx\"))\n",
    "print(suppl.sheet_names)\n",
    "suppl_human = suppl.parse(suppl.sheet_names[1])\n",
    "suppl_mouse = suppl.parse(suppl.sheet_names[2])\n",
    "suppl_human[\"organism\"] = \"human\"\n",
    "suppl_mouse[\"organism\"] = \"mouse\"\n",
    "frames = [suppl_human, suppl_mouse]\n",
    "suppl_df = pd.concat(frames)\n",
    "\n",
    "\n",
    "file = os.path.join(outputdir,'suppl_df.h5')\n",
    "with open(file, 'wb') as config_dictionary_file:\n",
    "    pickle.dump(suppl_df, config_dictionary_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a978247",
   "metadata": {},
   "source": [
    "# Plot sequences summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecebfd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_assays = suppl_df[suppl_df[\"organism\"] == \"human\"][\"assay_type\"]\n",
    "ordered_assays_full = suppl_df[suppl_df[\"organism\"] == \"human\"][\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f65dff",
   "metadata": {},
   "source": [
    "### How many tracks per assay type? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1cc1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(suppl_df[suppl_df[\"organism\"] == \"human\"].groupby(\"assay_type\").count()[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23504148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_evaluation_df(i,dataset_197k_evaluation, ordered_assays ):\n",
    "    # Create dataframe for plotting\n",
    "    df = pd.DataFrame()\n",
    "    # Add sequence\n",
    "    df[\"sequence\"] = np.repeat(i,len(ordered_assays))\n",
    "    # Add assay\n",
    "    df[\"assay\"] = ordered_assays\n",
    "    df[\"full\"] = ordered_assays_full\n",
    "    # Add pearson values \n",
    "    df[\"pearson\"] = (dataset_197k_evaluation[i][\"PearsonR\"])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb09fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_197k_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "for i in range(50):\n",
    "    df = get_sequence_evaluation_df(i,dataset_197k_evaluation, ordered_assays)\n",
    "    print(i)\n",
    "    final_df = pd.concat([final_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36568adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df[(final_df[\"assay\"]  == \"DNASE\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c48bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f21668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=tips, x=\"total_bill\", y=\"tip\", hue=\"assay\", style=\"time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c4e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df[\"sequence\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b46571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"ticks\", palette=\"pastel\")\n",
    "\n",
    "# Load the example tips dataset\n",
    "sns.violinplot(x=\"assay\", y=\"pearson\",  palette=\"mako\", data=final_df)\n",
    "\n",
    "sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3036dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset(\"penguins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f5f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now().time() # time object\n",
    "\n",
    "print(\"now =\", now)\n",
    "print(\"type(now) =\", type(now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b2c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.ecdfplot(data=final_df, x=\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ecc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous validation dictionary\n",
    "enformer_dict_file = os.path.join(outputdir,'00_enformer_dict_seqs.h5')\n",
    "\n",
    "with open(enformer_dict_file, 'rb') as config_dictionary_file:\n",
    "    human_validation_dict = pickle.load(config_dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe702d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_validation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fafb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b112c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- OLD "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd0b39",
   "metadata": {},
   "source": [
    "### PLOT: Distributions of pearson correlation coefficients per assay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming the values are in order of assay (TODO check) \n",
    "assay_list = list(suppl_df[\"assay_type\"])\n",
    "pearson_per_assay = list(metrics_human[0][\"PearsonR\"].numpy())\n",
    "data_tuples = list(zip(assay_list,pearson_per_assay))\n",
    "df_pearson_assay = pd.DataFrame(data_tuples, columns=['assay','pearson'])\n",
    "df_pearson_assay[\"pearson\"]\n",
    "df = df_pearson_assay\n",
    "df = df.astype({\"assay\": str, \"pearson\": float})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"assay\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bdad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "\n",
    "sns.set_theme(style=\"ticks\", palette=\"pastel\")\n",
    "\n",
    "# Load the example tips dataset\n",
    "\n",
    "# Draw a nested boxplot to show bills by day and time\n",
    "sns.violinplot(x=\"assay\", y=\"pearson\",  palette=\"mako\", linewidth=1.5,\n",
    "            data=df)\n",
    "sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ad2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Initialize \n",
    "g = sns.FacetGrid(df, row=\"assay\", hue=\"assay\", aspect=15, height=1, palette=\"mako\")\n",
    "\n",
    "# Draw the densities in a few steps\n",
    "g.map(sns.kdeplot, \"pearson\",\n",
    "      bw_adjust=.5, clip_on=False,\n",
    "      fill=True, alpha=1, linewidth=1.5)\n",
    "g.map(sns.kdeplot, \"pearson\", clip_on=False, color=\"w\", lw=2, bw_adjust=.5)\n",
    "\n",
    "# passing color=None to refline() uses the hue mapping\n",
    "g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n",
    "\n",
    "# Define and use a simple function to label the plot in axes coordinates\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.text(0, .2, label, fontweight=\"bold\", color=color,\n",
    "            ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "g.map(label, \"pearson\")\n",
    "\n",
    "# Set the subplots to overlap\n",
    "g.figure.subplots_adjust(hspace=-.3)\n",
    "\n",
    "# Remove axes details that don't play well with overlap\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[], ylabel=\"\")\n",
    "g.despine(bottom=True, left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a693d",
   "metadata": {},
   "source": [
    "# OLD (do not delete) - prepare  dictionary intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09dd801",
   "metadata": {},
   "source": [
    "## Check if the sequences are in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9bca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(human_sequences, memory_map=True, header=None, index_col=False, delimiter=\"\\t\")\n",
    "# keep only validation intervals \n",
    "validation_intervals= df[df[3]==\"valid\"]\n",
    "#validation_intervals = validation_intervals.head()\n",
    "# create list with interval\n",
    "interval_list = list()\n",
    "validation_intervals.apply(lambda row : interval_list.append(kipoiseq.Interval(row[0],row[1], row[2])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary for search (can be improved! quite slow)\n",
    "human_validation_dict = {}\n",
    "for interval in interval_list: \n",
    "    sequence = one_hot_encode(fasta_extractor.extract(interval))\n",
    "    human_validation_dict[interval] = sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "import pickle\n",
    "\n",
    "enformer_dict_file = os.path.join(outputdir,'00_enformer_dict_seqs.h5')\n",
    "# Step 2\n",
    "with open(enformer_dict_file, 'wb') as config_dictionary_file:\n",
    "    pickle.dump(human_validation_dict, config_dictionary_file)\n",
    "    \n",
    "# -------- read -------\n",
    "with open(enformer_dict_file, 'rb') as config_dictionary_file:\n",
    "    config_dictionary = pickle.load(config_dictionary_file)\n",
    "\n",
    "print(config_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a9eb58",
   "metadata": {},
   "source": [
    "### Same with mouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e04900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import kipoiseq\n",
    "from kipoiseq import Interval\n",
    "import pyfaidx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb45e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_sequences = \"/home/luisasantus/Desktop/crg_cluster/data/FED/basenji/mouse/mouse_sequences.bed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_file = \"/home/luisasantus/Desktop/crg_cluster/data/FED/hg38.fa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(mouse_sequences, memory_map=True, header=None, index_col=False, delimiter=\"\\t\")\n",
    "# keep only validation intervals \n",
    "validation_intervals= df[df[3]==\"valid\"]\n",
    "#validation_intervals = validation_intervals.head()\n",
    "# create list with interval\n",
    "interval_list = list()\n",
    "validation_intervals.apply(lambda row : interval_list.append(kipoiseq.Interval(row[0],row[1], row[2])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ff95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence):\n",
    "    return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastaStringExtractor:\n",
    "\n",
    "    def __init__(self, fasta_file):\n",
    "        self.fasta = pyfaidx.Fasta(fasta_file)\n",
    "        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n",
    "\n",
    "    def extract(self, interval: Interval, **kwargs) -> str:\n",
    "        # Truncate interval if it extends beyond the chromosome lengths.\n",
    "        chromosome_length = self._chromosome_sizes[interval.chrom]\n",
    "        trimmed_interval = Interval(interval.chrom,\n",
    "                                    max(interval.start, 0),\n",
    "                                    min(interval.end, chromosome_length),\n",
    "                                    )\n",
    "        # pyfaidx wants a 1-based interval\n",
    "        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n",
    "                                          trimmed_interval.start + 1,\n",
    "                                          trimmed_interval.stop).seq).upper()\n",
    "        # Fill truncated values with N's.\n",
    "        pad_upstream = 'N' * max(-interval.start, 0)\n",
    "        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n",
    "        return pad_upstream + sequence + pad_downstream\n",
    "\n",
    "    def close(self):\n",
    "        return self.fasta.close()\n",
    "\n",
    "\n",
    "def get_metadata(metadata):\n",
    "    with tf.io.gfile.GFile(metadata, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_dataset(tfr, metadata):\n",
    "\n",
    "    metadata = get_metadata(metadata)\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord, compression_type='ZLIB')\n",
    "\n",
    "    dataset = dataset.map(functools.partial(deserialize, metadata=metadata))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "fasta_extractor = FastaStringExtractor(fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13557e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary for search (can be improved! quite slow)\n",
    "mouse_validation_dict = {}\n",
    "for interval in interval_list: \n",
    "    sequence = one_hot_encode(fasta_extractor.extract(interval))\n",
    "    mouse_validation_dict[interval] = sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d9da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputdir = \"/home/luisasantus/Desktop/crg_cluster/data/FED/basenji/mouse\"\n",
    "\n",
    "enformer_dict_file = os.path.join(outputdir,'00_enformer_dict_seqs_mouse.h5')\n",
    "# Step 2\n",
    "with open(enformer_dict_file, 'wb') as config_dictionary_file:\n",
    "    pickle.dump(mouse_validation_dict, config_dictionary_file)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
